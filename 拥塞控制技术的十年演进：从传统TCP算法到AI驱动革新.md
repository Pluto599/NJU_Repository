# **拥塞控制技术的十年演进：从传统TCP算法到AI驱动革新**

## **摘要**

### **简要总结这项技术的动机（所要解决的问题）、目前的进展以及应用情况。**



## **引言**

### **试图解决什么问题？为什么问题很重要？解决该问题的关键挑战是什么？应对这些挑战的核心思路是什么？关键的设计或系统架构是什么？主要发现和评估结果是什么？**

​	近年来，互联网流量的爆炸式增长和新型应用场景的涌现，传统拥塞控制技术的局限性日益凸显。2016年之前，以CUBIC和Reno为代表的基于丢包反馈的TCP算法在高带宽、高延迟网络中表现不佳，无法有效应对视频流媒体、云计算等应用对网络性能的严苛要求。这些经典算法不仅在高带宽延迟积（BDP）环境下效率低下，还面临着与其他算法共存时的公平性问题。尽管早期出现了如DCTCP等改进方案，但其适用范围主要局限于数据中心短距离流量，难以满足更广泛的网络需求。

​	2016至2018年间，随着Google在SIGCOMM 2017上发布BBR算法，拥塞控制技术迎来了第一次范式转变。BBR主动测量瓶颈带宽和往返时间（RTT），摒弃了传统基于丢包的被动控制方式，显著提升了高BDP网络中的吞吐量和公平性。这一时期的技术突破主要受到广域网应用场景的驱动，特别是全球范围内视频流媒体服务的普及对网络稳定性提出了更高要求。与此同时，硬件技术的进步也为算法创新提供了支持，更精确的网络测量工具和更强大的计算能力使得像BBR这样需要复杂计算的算法得以实际部署。这一阶段的演进不仅解决了传统算法的关键缺陷，还为后续面向特定场景的优化奠定了基础。

​	2019至2021年，数据中心成为拥塞控制技术创新的主要试验场。随着RDMA技术在数据中心内部的普及，RoCEv2协议栈中的DCQCN和阿里巴巴的HPCC等算法应运而生。这些方案通过显式拥塞通知（ECN）和精细化的速率控制，专门针对数据中心内部的高吞吐、低延迟需求进行了优化。这一时期的算法设计开始深度绑定特定网络环境和业务需求。与此同时，可编程交换机等新型网络硬件的出现，使得更细粒度的流量控制成为可能，进一步推动了拥塞控制技术从通用向专用方向的演进。

​	2022至2025年，人工智能技术的快速发展为拥塞控制领域带来了新的挑战和机遇。分布式AI训练对网络延迟提出了前所未有的严苛要求，促使研究人员探索将带内遥测（INT）技术与强化学习等AI方法相结合的创新方案。Meta在SIGCOMM 2023上发布的ACC算法就是这一趋势的典型代表，它通过实时网络状态感知和动态策略调整，实现了AI计算集群中的超低延迟通信。这一阶段的技术演进呈现出明显的智能化特征，传统启发式算法正在被数据驱动的方法所取代。

​	展望未来，随着6G技术和算力网络的发展，拥塞控制技术将继续向自适应、智能化的方向深化，其研究边界也将从单纯的网络性能优化扩展到与计算、存储资源的协同调度，展现出更广阔的发展前景。

## **具体技术**

### **针对该技术需描述：（1）核心思想；（2）与前面工作的关系；（3）更先进的部分；（4）具体实现细节和关键点；（5）评估 1）实验设置。描述如何进行实验；2）实验结果总结。**

- **2016-2018**：传统TCP算法（CUBIC）的局限性暴露，BBR算法（Google）横空出世（SIGCOMM 2017）。
- **2019-2021**：数据中心场景驱动创新，DCQCN（RoCE）、HPCC（Alibaba）成为主流（NSDI 2019）。
- **2022-2025**：AI网络催生超低延迟需求，INT（带内遥测）与强化学习结合（如ACC（Meta, SIGCOMM 2023））。

### 传统TCP拥塞控制算法

#### 核心思想

​	TCP拥塞控制算法作为保障互联网稳定运行的核心机制，通过动态调整发送窗口平衡网络带宽与传输延迟，保证了发送方充分利用网络带宽，防止因发送方数据注入速率过快而导致的网络过载。从早期基础的TCP Tahoe到引入快速恢复机制的TCP Reno，再到IETF标准化的RFC5681，直至适应高带宽延迟积网络的CUBIC算法。

#### 具体实现

##### RFC5681

​	以RFC5681为例，它的核心机制是**拥塞窗口（Congestion Window, `cwnd`）**，它限制了发送方在收到确认前可以发送的数据量。`cwnd`的调整主要遵循以下四个经典算法：

1. **慢启动 (Slow Start)** ：为在连接建立初期快速利用可用带宽，`cwnd`以指数速率增长。每当收到一个确认（ACK），`cwnd`便增加一个最大报文段长度（MSS）。
	$$
	cwnd←cwnd+1×MSS
	$$
	这使得`cwnd`在每个往返时间（RTT）内近似翻倍，迅速逼近网络容量。

2. **拥塞避免 (Congestion Avoidance)** ：当`cwnd`增长到预设的**慢启动阈值（`ssthresh`）**后，为避免过激增长导致拥塞，算法进入拥塞避免阶段。此时，`cwnd`的增长方式由指数变为线性，更加审慎地探测网络。其增长速度约为每个RTT增加一个MSS。
	$$
	cwnd←cwnd+MSS×MSScwnd
	$$

3. **快速重传 (Fast Retransmit)** ：当发送方连续收到三个对于同一数据包的重复ACK时，它会立即重传丢失的报文段，而无需等待重传计时器超时。该机制显著减少了因单个报文段丢失而造成的延迟。

4. **快速恢复 (Fast Recovery)** ：在快速重传之后，TCP将`ssthresh`减半，并将`cwnd`设置为新的`ssthresh`值，而不是重置为1个MSS，随即进入拥塞避免阶段。 
	$$
	ssthresh \leftarrow \frac{cwnd}{2} \ cwnd \leftarrow ssthresh
	$$
	这使得TCP在经历丢包后能维持较高的吞吐量，避免了慢启动导致的传输速率骤降。

​	该标准强调了**加性增/乘法减（AIMD）**原则的核心地位：在无拥塞时线性增加窗口，在拥塞时乘性减小窗口，以维持公平和效率的平衡。RFC5681为不同操作系统的TCP实现提供了统一的基础框架，确保了互操作性。

##### CUBIC

​	随着网络带宽和高带宽延迟积（BDP）的持续增长，RFC5681的线性增长机制在恢复大窗口时变得极其缓慢。CUBIC算法的核心思想是将窗口增长独立于RTT，仅作为自上次拥塞事件后时间t*t*的函数，实现更快的探测速度和更好的RTT公平性。其关键在于通过用**三次函数（Cubic Function）的增长模型替代了拥塞避免阶段的线性增长**：
$$
W_{cubic}(t) = C(t-K)^3 + W_{max}
$$
其中 `W_max` 是发生拥塞事件前的最大窗口值，`C` 是可调缩放因子（默认0.4），`K` 是计算所得的时间偏移量：`K = ((W_max * β) / C)^(1/3)`（`β` 为乘法减因子，默认0.7）。

​	该函数在 `t=K` 处取得拐点，形成S形曲线：在 `t<K` 时凸增长（快速探测），在 `t>K` 时凹增长（平稳接近 `W_max` ）。当 `W(t)` 接近 `W_max` 时，函数进入平稳期进行带宽探测。检测到拥塞事件（丢包）时，执行乘法减：`W_max ← W_current`, `W_current ← W_current * β`，并重置计时器 `t=0`。

​	CUBIC还包含一个TCP友好区域，当 `W_current` 远小于 `W_max` 时（如初始启动），使用标准Reno的慢启动和拥塞避免算法（`cwnd ← cwnd + 1/cwnd`）以确保与标准TCP的共存公平性。通过将窗口增长与时间而非RTT关联，CUBIC在高BDP网络中实现了比Reno快数十倍的窗口恢复速度，显著提升了吞吐量，同时维持了良好的公平性和稳定性，成为现代高速网络的事实标准。

#### 实验结果

待插入图片

### BBR

#### 前面工作的不足

​	传统TCP拥塞控制算法主要依赖于一个核心机制：基于丢包的拥塞检测。这些算法一旦检测到丢包，便会判定网络发生了拥塞，并迅速降低发送速率。它们的根本缺陷在于将丢包作为拥塞的唯一信号。然而，在当今具有大缓存的现代网络设备中，数据包只有在路由器或交换机的缓冲区被完全填满后才会开始丢失。此时，网络延迟已经显著增加，用户体验早已恶化。换言之，当传统算法检测到拥塞时，拥塞已经持续了一段时间。此外，在无线网络等高噪声环境中，数据包丢失可能仅仅是由于信号干扰而非网络拥塞，但基于丢包的算法仍会错误地降低速率，导致带宽资源未被充分利用。

这种设计在早期网络设备缓冲区较小的环境下有效，但随着网络技术进步（如NIC带宽从Mbps提升到Gbps、内存容量指数级增长），该机制暴露出严重缺陷：

1. **高缓冲区导致Bufferbloat**：当瓶颈链路缓冲区较大时，传统算法持续填满缓冲区，造成RTT增长到秒级而非毫秒级。这种现象被称为"Bufferbloat"，会导致交互式应用的延迟急剧增加（如网页加载、视频通话卡顿）。
2. **低缓冲区吞吐量下降**：当缓冲区小于带宽时延积（BDP=Bandwidth×Delay Product）时，少量丢包即会触发不必要的降速，使得实际吞吐量远低于链路容量。
3. **无法区分拥塞类型**：传统机制无法区分真实的网络拥塞（带宽不足）和瞬时队列波动，导致带宽利用率与延迟难以平衡。

#### 创新点

​	BBR（Bottleneck Bandwidth and Round-trip propagation time）算法相较于传统TCP算法，其先进之处在于它采用了全新的拥塞控制思路。它不依赖于丢包来判断拥塞，而是通过主动测量网络的两个关键参数——瓶颈带宽（Bottleneck Bandwidth, BtlBw）和往返传播时延（Round-trip propagation time, RTprop）——来调节发送速率。

​	BBR构建了一个明确的网络路径模型，旨在找到既能最大化吞吐量又能最小化延迟的最佳工作点，即**克莱因罗克最优点（Kleinrock’s optimal operating point）**。通过持续测量和更新这两个核心参数，BBR能够精确地控制在途数据量（inflight data），使其刚好等于网络的带宽时延积（Bandwidth-Delay Product, BDP）。这样，它既能充分利用链路的全部带宽，又能避免在网络节点中产生不必要的排队，从而显著降低了延迟。因此，BBR能够同时实现高吞吐和低延迟，尤其是在存在深度缓冲（bufferbloat）或随机丢包的网络环境中，其性能优势尤为明显。

#### 核心思想

​	BBR算法围绕“逼近 Kleinrock 最佳工作点”这个核心目标展开。它首先需要估算出 BDP，其计算公式为：
$$
BDP=BtlBw×RTprop
$$

- **BtlBw (瓶颈带宽)**：是整个网络路径中，链路容量最小的那一段的带宽。这是发送方能够达到的最大数据传输速率。
- **RTprop (往返传播时延)**：是没有经过任何缓冲区排队时，一个数据包从发送方到接收方再返回的固有延迟。

估算出这两个值后，BBR 为了填满网络管道而不产生排队，它需要维持的在途数据量恰好为 BDP。BBR 通过控制其拥塞窗口 (cwnd) 和发包节奏 (pacing rate) 来实现这一目标：
$$
\text{pacing\_rate} = \text{gain} \times \text{BtlBw}
\\
\text{cwnd} = \text{gain} \times \text{BDP} 
$$
此处 `gain` 是一个增益系数，BBR 通过动态调整这个系数，在不同的状态下实现不同的目标。

#### 具体实现

​	为了精确地测量 BtlBw 和 RTprop 并动态适应网络变化，BBR 设计了一个状态机，主要包含以下四个状态：

1. Startup (启动阶段)

​	当一个连接刚开始时，BBR 处于 Startup 状态，其目标是快速探测并逼近网络的瓶颈带宽。在此阶段，它的 `pacing_gain` 和 `cwnd_gain` 都被设置为一个较高的值（如 2.89）。此时发送速率呈指数级增长，每经过一个 RTT拥塞窗口就会翻倍。BBR 会持续监控 RTT 的变化，一旦发现 RTT 开始稳定增长，就意味着数据包开始在瓶颈路由器的缓冲区中排队了，此时网络管道已被填满。这表明 BBR 已经找到了大致的瓶颈带宽，于是它会退出 Startup 阶段，进入 Drain 阶段。

2. Drain (排空阶段)

​	在 Startup 阶段的末期，为了填满管道，BBR 向网络中注入了超过 BDP 的数据包，导致了缓冲区排队。Drain 阶段的目标就是排空这些多余的数据包，降低延迟。在此阶段，BBR 会使用一个小于 1 的 `gain`（即 Startup 增益的倒数），主动降低发送速率，直到在途数据量降低到等于估算出的 BDP 为止。这个过程完成后， BBR 就进入了主要的稳态——ProbeBW。

3. ProbeBW (带宽探测阶段)

​	这是 BBR 连接在大部分时间所处的稳态。其核心目标是：在维持高吞吐量和低延迟的同时，周期性地探测是否存在更大的可用带宽。

​	ProbeBW 阶段采用了增益循环 (gain cycling) 机制。这个循环由 8 个小周期组成，每个周期持续一个 RTT。在一个完整的循环中，`pacing_gain` 的值会依次在 `[1.25, 0.75, 1, 1, 1, 1, 1, 1]` 这个序列中变化：

- **探测带宽 (gain = 1.25)**：在第一个周期，BBR 会短暂地将发送速率提升 25%。如果网络有更多可用带宽，这次探测将会成功，BBR 会观察到更高的传输速率，并更新其 BtlBw 估值。
- **排空队列 (gain = 0.75)**：紧接着，BBR 会在下一个周期将发送速率降低 25%，以排空上一步探测时产生的微小队列，使延迟恢复到基本水平。
- **巡航 (gain = 1)**：在剩下的 6 个周期里，BBR 以估算出的瓶颈带宽速率平稳地发送数据，充分利用带宽。

​	通过这个 8 周期的循环，BBR 实现了在不产生持续性排队（Bufferbloat）的前提下对网络带宽变化做出灵敏的响应。

4. ProbeRTT (延迟探测阶段)

​	BBR 需要一个精确的 RTprop 估值，但如果在 ProbeBW 阶段，网络中一直存在少量排队，那么测量的 RTT 就会一直偏高，导致 RTprop 的估值不准。为了解决这个问题，当 BBR 发现 RTprop 的估值在一段时间内（例如 10 秒）没有更新时，它会主动进入 ProbeRTT 状态。

​	在此状态下，BBR 会将拥塞窗口缩减到一个极小的值（例如 4 个数据包），并维持至少 200 毫秒。这个操作会清空网络路径上几乎所有的缓冲区队列，从而让 BBR 能够测量到一个接近物理极限的往返传播时延，并更新其 RTprop 估值。之后，BBR 会根据最新的 RTprop 重新进入 Startup 或 ProbeBW 状态。

#### 实验结果

待插入图片

### DCQCN

#### 前面工作的不足

​	BBR算法通过精确测量网络的瓶颈带宽和往返传播时间来来调整发送速率，但其核心假设基于存在丢包的传统TCP/IP网络环境。然而，因为BBR假定瓶颈是持续存在的，所以在网络路径的瓶颈链路并非单一且固定的复杂场景中，BBR的带宽探测机制可能会导致不准确的测量结果。此外，当多个BBR流在同一链路上竞争时，可能会因为其激进的带宽探测行为而导致不公平的带宽分配，某些流可能会抢占过多的带宽，而另一些流则会“饿死”。更重要的是，BBR对于网络中由交换机明确发出的拥塞信号（例如ECN标记）并不直接响应，它主要依赖于自己对网络状态的测量和建模，这使得它在能够提供显式拥塞通知（Explicit Congestion Notification, ECN）的现代数据中心网络中，无法充分利用硬件提供的拥塞信息，从而影响了其反应速度和控制精度。

#### 创新点

​	与BBR相比，DCQCN（Data Center Quantized Congestion Notification）算法展现出了其在现代数据中心网络，尤其是基于远程直接内存访问（Remote Direct Memory Access, RDMA）的高速网络环境中的优越性。DCQCN最核心的优势在于它与硬件的能力充分结合，利用交换机提供的显式拥塞通知（ECN）来快速、精确地感知网络拥塞。当交换机检测到队列长度超过某个阈值时，会主动标记数据包的ECN位。接收端在收到被标记的包后，会生成一个拥塞通知包（Congestion Notification Packet, CNP）并回送给发送端。这种机制使得发送端几乎可以瞬时了解到网络路径上的拥塞状况，而不需要像BBR那样通过一个探测周期来推断网络状态。因此，DCQCN反应更为迅速，能够有效抑制网络延迟和抖动，避免了BBR在探测带宽时可能引入的额外延迟。同时，DCQCN的控制逻辑是基于一个精确的反馈循环——发送端根据收到的CNP频率——来调整其发送速率，这种基于速率的控制方式相比BBR基于模型的控制，对网络动态变化的适应性更强，也更容易实现公平性。

#### 核心思想

​	DCQCN算法的核心思想是构建一个**基于硬件的、显式的、端到端的闭环拥塞控制系统**。它的基本逻辑是：网络设备（交换机）作为拥塞的“第一发现者”，主动将拥塞信号（ECN标记）通知给通信的终端（接收端），接收端再将这个信号明确地传递回发送端，最终发送端根据收到的拥塞信号，以直接调节发送速率的方式来精确地控制流量。这个由“交换机标记 -> 接收端通知 -> 发送端调节”构成的紧密反馈闭环，旨在实现**极低的延迟、快速的收敛和高效的带宽利用**，尤其适用于对延迟极其敏感的RDMA网络环境。

#### 具体实现

​	DCQCN算法算法的控制机主要包含三个核心状态，是DCQCN能够快速响应并有效控制拥塞的关键：

1. **速率增加（Rate Increase）**

​	在网络路径未出现拥塞时，发送端为了主动探测并利用潜在的可用带宽，会周期性地、线性地增加其发送速率。与TCP基于ACK的窗口增长方式不同，DCQCN的速率控制器基于定时器。每经过一个固定的时间周期，如果收到来自接收端的拥塞通知（CNP），就切换到速率降低状态；如果没有收到来自接收端的CNP，发送端就会将当前速率增加一个固定的加性速率（`additive_rate`），同时确保速率不超过线路最大速率。
$$
\text{current\_rate} =min(\text{current\_rate} + \text{additive\_rate},\, \text{line\_rate})
$$
这种平缓的增长方式旨在温和地填充网络管道，直至逼近网络容量的物理极限，从而在确保低延迟的同时最大化吞吐量。

2. **速率降低（Rate Decrease）**

​	这个状态的核心使命是快速、显著地降低发送端的速率，以立刻缓解已经发生的网络拥塞。一旦进入这个状态，状态机会立即记录下执行速率降低时的时间戳或发送的字节数，并执行一个“乘性减小”（Multiplicative Decrease）操作：
$$
 current\_rate = current\_rate \times  (1 - \alpha)
$$
这个操作的目的是为了给网络足够的时间来消化和清除积压在交换机队列中的数据包。在完成速率削减之后，状态机会立刻转换到快速恢复状态。

3. **快速恢复（Fast Recovery）**

​	在执行了乘性减小之后，为防止系统因过度反应而产生振荡，DCQCN会进入一个短暂的“冷静期”，即快速恢复阶段。发送端会将其速率维持在降低后的水平，并在此期间忽略任何新收到的CNP。这是因为这些后续的CNP很可能是由同一次拥塞事件所引发的，在速率降低之前就已经在网络中传播。发送端会等待一个恢复周期（通常为一个RTT），以确保网络状态已经稳定。恢复周期结束后，如果没有新的拥塞信号，发送端将重新进入速率增加阶段，开始新一轮的带宽探测；否则保持当前速率不变，继续等待。这种机制确保了算法在拥塞后的稳定性和鲁棒性，避免了速率的剧烈波动。

#### 实验结果

待添加图片

### HPCC

#### 前面工作的不足

​	DCQCN算法是专为RDMA网络设计的拥塞控制算法，它在一定程度上解决了无损网络中的拥塞问题，但其设计存在一些局限性。首先，DCQCN的信令机制相对粗糙。它依赖于交换机设置ECN标记，这本质上是一个二进制信号，只能通知发送端“有拥塞”或“无拥塞”，却无法量化拥塞的严重程度。这种信息的缺乏导致发送端在调整速率时难以做出精确判断，从而引发发送速率的持续振荡，收敛速度较慢。其次，DCQCN的性能表现高度依赖于一系列精心设计的参数，这些参数的整定过程非常复杂，且在网络负载、拓扑结构或链路带宽动态变化的环境中，固定的参数往往难以维持最优性能，适应性较差。最后，由于其反应机制的滞后性和不精确性，DCQCN为了防止丢包，通常需要在交换机上维持一个较大的缓冲区来吸收突发流量，这不仅增加了网络延迟，也浪费了宝贵的芯片上缓存资源。

#### 创新点

​	相较于DCQCN，HPCC（High Precision Congestion Control）算法的最大优势在于其“高精度”的拥塞感知能力。HPCC不再依赖于模糊的ECN信号，而是利用现代可编程交换机的带内网络遥测（INT）功能，从根本上改变了拥塞控制的基础。交换机可以将精确的元数据，包括数据包到达的时间戳（timestamp）、离开交换机时的队列长度（queue length）以及该端口已发送的总字节数（tx_bytes），直接嵌入到正在传输的数据包头部。当这些信息随数据包到达接收端后，接收端再通过ACK将其反馈给发送端。这样一来，发送端就能获得关于网络路径、特别是瓶颈链路的极其精细的负载信息。这种精确的数据使得HPCC能够迅速而准确地计算出瓶颈链路的可用带宽，从而将发送速率调整到恰到好处的水平。因此，HPCC能够以极快的速度收敛到公平共享速率，几乎完全消除了网络中的排队延迟，实现了近乎为零的交换机缓冲区占用，同时保持了极高的链路利用率。这种基于精确测量的控制方式，也使得HPCC摆脱了对复杂参数调优的依赖，使其在高度动态的数据中心环境中表现得更加稳健和高效。

#### 核心思想

​	HPCC算法的核心思想是构建一个基于精确测量的闭环控制系统，使发送端的发送速率（`rate`）精确匹配网络瓶颈的可用容量（`C`），从而在最大化吞吐量的同时，将网络中的在途数据量维持在一个极低的水平。它将拥塞控制问题转化为一个经典的控制论问题：如何根据实时的、精确的反馈来调整输入（发送速率），以稳定输出（网络状态）。算法的基石是利用INT提供的`tx_bytes`（传输字节数）和`timestamp`（时间戳）来计算瓶颈链路的实际线速 `C`。一旦精确获知了 `C`，HPCC的目标就是让发送速率略低于 `C`，同时确保在途数据量仅略大于带宽延迟积（Bandwidth-Delay Product, BDP），以“填满”管道但又不产生额外的排队。为此，它定义了一个目标在途数据量，该数据量由一个基础值（等于BDP）和一个小的额外量组成，这个额外量用于驱动控制循环。发送端持续监控实际的在途数据量，并与目标值进行比较，通过一个控制增益因子 `η` 来平滑地调整发送速率，最终实现速率与链路容量的精确匹配。

#### 具体实现

​	HPCC的算法围绕一个状态机和一组控制方程展开，确保在不同阶段都能高效地利用网络资源。其核心速率控制方程为： `rate = rate + η * (B_target - inflight_bytes_actual)` 其中，`η` 是控制增益，`B_target` 是目标在途数据量，`inflight_bytes_actual` 是当前实际的在途数据量。`B_target` 的计算与瓶颈链路容量 `C` 和往返时延 `RTT` 相关，通常设为 `C * RTT` 乘以一个略大于1的系数。

​	为了在启动阶段快速抢占带宽并准确测量链路容量，HPCC设计了一个三阶段的状态机：

1. **加速（ACCELERATE）状态**

​	这是连接的初始状态。在此阶段，发送端采用类似TCP慢启动的方式，以指数形式快速增加发送速率，从而迅速达到网络可用带宽的量级。速率更新公式为：
$$
rate = rate * (1 + α)
$$
其中 `α` 是一个大于0的增长因子。当发送端检测到在途数据量超过一个预设的阈值时，意味着数据流可能已经接近或达到了瓶颈链路的容量，此时状态机便从**加速**状态切换到**探测**状态。

2. **探测（PROBE）状态**

​	进入此状态的目的是为了精确地测量出瓶颈链路的线速 `C`。发送端会发送一个或多个大小固定的“探测”数据块（probe transaction），并利用INT记录下这些数据块在瓶颈交换机入口和出口的时间戳和传输字节数。通过计算 `Δtx_bytes / Δtimestamp`，就可以非常精确地计算出链路的实际容量 `C`。这个测量过程非常短暂，一旦完成，发送端就获得了后续进行精细速率调节所必需的关键信息，切换到**稳定**状态。

3. **稳定（STABLE）状态**

​	这是HPCC运行的主要状态。此时，发送端已经拥有了瓶颈链路容量 `C` 的精确值。它使用上面提到的核心控制方程，进行精细的、线性的速率调整。速率的调整基于当前在途数据量与目标在途数据量之间的差值。这个差值反映了当前速率与理想速率的偏离程度，算法会进行平滑的加性增减（Additive Increase/Additive Decrease, AIAD），使发送速率始终紧密地围绕着链路的实际容量 `C` 波动，从而在实现接近100%链路利用率的同时，将交换机内的排队延迟控制在微秒级别。如果网络路径发生变化（例如，通过RTT的大幅增加检测到），导致瓶颈改变，状态机会重新切换回**加速**状态，以启动新一轮的带宽发现和容量测量。

#### 实验结果

待添加图片

### ACC

#### 前面工作的不足

​	HPCC作为一种为高速数据中心网络设计的拥塞控制算法，其核心思想是利用In-band Network Telemetry（INT）技术来获取精确的链路负载信息，从而计算出允许的发送速率。这种设计虽然在精确控制方面取得了显著进展，但是它的有效性高度依赖于ECN标记阈值的精确配置。在实际的数据中心环境中，网络流量具有高度的动态性和突发性，这使得为交换机设置一个全局统一且在所有情况下都最优的静态ECN阈值变得几乎不可能。如果阈值设置得过低，即使网络负载尚有余量，交换机也会过早地标记数据包，导致链路利用率不足；反之，如果阈值设置得过高，网络可能会在交换机做出反应之前就发生拥塞，从而引发数据包丢失和网络性能急剧下降。HPCC本身缺乏一种能够根据实时网络状况动态调整此阈值的机制，这使得它在面对复杂多变的网络流量模式时，难以持续保持最佳性能。

#### 创新点

​	相较于HPCC，ACC（Automatic Congestion Control）展现了其更强的自适应性和智能化。ACC最核心的进步在于它引入了一套自动化的ECN标记阈值调整机制，从而摆脱了HPCC对静态、预设阈值的依赖。ACC不再将ECN阈值视为一个一成不变的常量，而是将其看作一个需要根据实时网络反馈进行动态优化的变量。它通过持续监控关键的网络性能指标，如队列长度的变化率和链路利用率，来智能地判断当前阈值是否合适。当检测到网络即将进入拥塞状态的早期信号时，ACC能够主动地、增量式地调整阈值，而不是等到拥塞发生后才被动响应。这种前瞻性的调整机制使得网络能够始终运行在一个接近理想状态的临界点，既能最大化吞吐量，又能有效避免拥塞导致的延迟和丢包。因此，ACC通过将控制逻辑从简单的被动响应升级为主动的、基于预测的动态调优，实现了在复杂网络环境下的鲁棒性和高效性。

#### 核心思想

​	ACC算法的核心思想在于实现一个闭环反馈控制系统，该系统能够根据网络状态的细微变化，自动且精细地调整交换机上的ECN标记阈值，从而将网络维持在“无损”且“高吞吐”的最佳工作区间。ACC持续监控两个关键的遥测指标：队列长度（Queue Length）和发送速率（Tx Rate），从而感知队列的增长趋势、了解链路的繁忙程度。ACC力求将队列长度的增长率稳定在零附近，达到一种动态平衡。如果队列增长过快，那么现有阈值过高，有潜在的拥塞风险，ACC会降低阈值以提前发出警告；如果队列持续为空或减少，且链路未被充分利用，那么阈值可能过低，限制了性能，ACC则会提高阈值以鼓励更高的发送速率。通过这种方式，ACC将拥塞控制的焦点从“管理队列长度”转移到了“管理队列动态”，从而实现对网络拥塞更加灵敏和精确的控制。

#### 具体实现

​	ACC算法的具体实现围绕一个精心设计的状态机来展开，该状态机定义了ECN阈值调整的完整逻辑。这个状态机包含三个核心状态：

1. **稳定状态（STABLE）**

​	这是系统的初始和默认状态。在此状态下，ACC认为当前的ECN阈值相对合适，系统运行平稳。它会持续监控网络的队列长度。只要队列长度保持在一个非常低的水平（例如持续为零），并且这种情况持续了一段预设的时间（T_stable），状态机就会判断当前阈值可能过于保守，限制了网络性能，于是切换到**探测状态（PROBE）**以尝试提升阈值。

2. **探测状态（PROBE）**

​	进入此状态是为了探索更高的链路利用率。ACC会逐步增加ECN阈值，其增加的步长（step）和频率由预设参数控制。每次增加阈值后，算法会进入一个观测周期（T_probe），用于评估此次调整带来的效果。在探测期间，如果检测到队列长度开始出现持续的、非零的增长，这表明已经触及了网络的性能拐点，继续增加阈值将引发拥塞。此时，状态机会立即从**探测状态（PROBE）切换到调整状态（ADJUST）**，以期找到新的平衡点。如果在整个探测周期内队列长度始终保持为零，算法会认为网络仍有余量，并继续留在探测状态，进行下一轮的阈值提升。

3. **调整状态（ADJUST）**

​	此状态的目标是精确地找到并维持最佳的ECN阈值。当系统从探测状态切换而来时，意味着队列已经开始形成。在调整状态下，ACC采用一种更为精细的反馈控制算法来动态调整阈值。它会根据队列长度的变化率来计算阈值的调整量。这个调整过程可以用一个积分控制器来描述：

$$
Knew=Kold−α×(qlen−qtarget)
$$

​	其中，Knew 是新的ECN阈值，Kold 是旧的阈值，qlen 是当前测量的队列长度，qtarget 是一个理想的目标队列长度（通常设定为一个较小的值，以允许一定的缓冲），α 是一个控制增益系数，它决定了控制器对队列长度变化的响应速度。这个公式使得当实际队列长度超过目标值时，阈值会相应降低，以产生更多的ECN信号来抑制发送速率；反之，当队列长度低于目标值时，阈值会升高，以允许更高的吞吐。当队列长度稳定在目标值附近，并且这种情况持续了一段时间（T_adjust）后，ACC认为已经找到了新的稳定工作点，状态机便会切换回**稳定状态（STABLE）**，完成一个完整的闭环控制周期。

#### 实验结果

待添加图片

## 结论



## 参考文献

Jacobson, V. (1988). Congestion avoidance and control. *ACM SIGCOMM Computer Communication Review*, *18*(4), 314–329. https://doi.org/10.1145/52325.52356



Allman, M., Paxson, V., & Blanton, E. (2009). *TCP Congestion Control* (RFC5681; 页 RFC5681). RFC Editor. https://doi.org/10.17487/rfc5681



Ha, S., Rhee, I., & Xu, L. (2008). CUBIC: A new TCP-friendly high-speed TCP variant. *ACM SIGOPS Operating Systems Review*, *42*(5), 64–74. https://doi.org/10.1145/1400097.1400105



Cardwell, N., Cheng, Y., Gunn, C. S., Yeganeh, S. H., & Van Jacobson. (2017). BBR: Congestion-based congestion control. *Communications of the ACM*, *60*(2), 58–66. https://doi.org/10.1145/3009824



Zhu, Y., Eran, H., Firestone, D., Guo, C., Lipshteyn, M., Liron, Y., Padhye, J., Raindel, S., Yahia, M. H., & Zhang, M. (2015). Congestion Control for Large-Scale RDMA Deployments. *Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication*, 523–536. https://doi.org/10.1145/2785956.2787484



Li, Y., Miao, R., Liu, H. H., Zhuang, Y., Feng, F., Tang, L., Cao, Z., Zhang, M., Kelly, F., Alizadeh, M., & Yu, M. (2019). HPCC: High precision congestion control. *Proceedings of the ACM Special Interest Group on Data Communication*, 44–58. https://doi.org/10.1145/3341302.3342085



Yan, S., Wang, X., Zheng, X., Xia, Y., Liu, D., & Deng, W. (2021). ACC: Automatic ECN tuning for high-speed datacenter networks. *Proceedings of the 2021 ACM SIGCOMM 2021 Conference*, 384–397. https://doi.org/10.1145/3452296.3472927